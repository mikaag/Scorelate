{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-19T15:14:11.075816Z",
     "end_time": "2023-04-19T15:14:13.456807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tempfile\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import metrics\n",
    "import pymssql\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "import warnings\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mpl_toolkits import mplot3d\n",
    "import yaml\n",
    "from scipy.stats import pearsonr\n",
    "import decimal\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "random_state = 2023\n",
    "\n",
    "# Preset matplotlib figure sizes.\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
    "\n",
    "print(tf.__version__)\n",
    "# To make the results reproducible, set the random seed value.\n",
    "tf.random.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import decimal\n",
    "import math\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from matplotlib import pyplot\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import pymssql as pss\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DatabaseLoader:\n",
    "    def __init__(self, _server_info):\n",
    "        _conn = pss.connect(server=_server_info['name'], user=_server_info['user'],\n",
    "                                password=_server_info['password'], database=_server_info['database'])\n",
    "\n",
    "        self._cursor = _conn.cursor()\n",
    "        self._empty = True\n",
    "        self.df = None\n",
    "        self._global_primary = None\n",
    "        self._global_secondary = None\n",
    "        self._global_primary_type = None\n",
    "        self._global_secondary_type = None\n",
    "        self._target = None\n",
    "        self._special_target = None\n",
    "        self._model = None\n",
    "\n",
    "    def get_column_index(self, _column_name):\n",
    "        for index, col in enumerate(self._cursor.description):\n",
    "            if col[0] == _column_name:\n",
    "                return index\n",
    "\n",
    "    def replace_col_name(self, _cur_headers, _old, _new):\n",
    "        for idx, header in enumerate(_cur_headers):\n",
    "            if header == _old:\n",
    "                _cur_headers[idx] = _new\n",
    "\n",
    "    def load_tables(self, _tables, _ignore=None):\n",
    "        for _table in _tables:\n",
    "            self.load_table(_table, ignore=_ignore)\n",
    "\n",
    "    def load_table(self, _table_info, ignore=None):\n",
    "        if ignore is None:\n",
    "            ignore = []\n",
    "        self._cursor.execute(\"SELECT * FROM {table}\".format(table=_table_info['name']))\n",
    "\n",
    "        _headers = []\n",
    "        _rows = []\n",
    "\n",
    "        for entity in self._cursor.description:\n",
    "            _headers.append(entity[0])\n",
    "\n",
    "        _primary_index = self.get_column_index(_table_info['primary'])\n",
    "        _secondary_index = self.get_column_index(_table_info['secondary'])\n",
    "\n",
    "        _res = self._cursor.fetchone()\n",
    "        while _res:\n",
    "            _row = [element for element in _res]\n",
    "            if _row[_primary_index] is not None and _row[_primary_index].strip() != \"n/a\" and _row[_secondary_index] != \\\n",
    "                    _table_info['null_secondary']:\n",
    "                # Strips trailing whitespace from strings\n",
    "                for _idx, item in enumerate(_row):\n",
    "                    if isinstance(item, str):\n",
    "                        _row[_idx] = item.strip()\n",
    "\n",
    "                    if isinstance(item, decimal.Decimal):\n",
    "                        _row[_idx] = float(item)\n",
    "\n",
    "                _rows.append(_row)\n",
    "            _res = self._cursor.fetchone()\n",
    "\n",
    "        if self._empty:\n",
    "            self._empty = False\n",
    "            self.df = pd.DataFrame(_rows, columns=_headers)\n",
    "            self._global_primary = _table_info['primary']\n",
    "            self._global_secondary = _table_info['secondary']\n",
    "            self._global_primary_type = type(self.df[self._global_primary].iloc[0])\n",
    "            self._global_secondary_type = type(self.df[self._global_secondary].iloc[0])\n",
    "        else:\n",
    "            # Replace the primary and secondary key column names with the global primary and secondary key names\n",
    "            self.replace_col_name(_headers, _table_info['primary'], self._global_primary)\n",
    "            self.replace_col_name(_headers, _table_info['secondary'], self._global_secondary)\n",
    "\n",
    "            temp_df = pd.DataFrame(_rows, columns=_headers)\n",
    "            # Cast the primary and secondary keys to global type\n",
    "            temp_df[self._global_primary] = temp_df[self._global_primary].astype(self._global_primary_type)\n",
    "            temp_df[self._global_secondary] = temp_df[self._global_secondary].astype(self._global_secondary_type)\n",
    "\n",
    "            self.df = pd.merge(temp_df, self.df, how='inner', on=['VIN', 'CreaditScore'])\n",
    "\n",
    "        # Remove any columns to ignore\n",
    "        self.df = self.df.drop(labels=ignore, axis=1, errors='ignore')\n",
    "\n",
    "    def dropna(self):\n",
    "        # Drop any rows with NaN values\n",
    "        self.df.dropna(how='any', inplace=True)\n",
    "\n",
    "    def move_col_to_end(self, col):\n",
    "        _cols = list(self.df.columns)\n",
    "        _cols.remove(col)\n",
    "        _cols.append(col)\n",
    "        self.df = self.df[_cols]\n",
    "\n",
    "    def prune_for_target(self, _prediction_info):\n",
    "        self._target = _prediction_info['target']\n",
    "        _replace_guide = _prediction_info['replace']\n",
    "\n",
    "        self.df = self.df[self.df[self._target].isin(list(_replace_guide.keys()))]\n",
    "        for key, value in _replace_guide.items():\n",
    "            self.df[self._target].replace(to_replace=key, value=value, inplace=True)\n",
    "\n",
    "        # Move the target to the end of the dataframe\n",
    "        _cols = list(self.df.columns)\n",
    "        _cols.remove(self._target)\n",
    "        _cols.append(self._target)\n",
    "        self.df = self.df[_cols]\n",
    "\n",
    "        if _prediction_info['doSpecial']:\n",
    "            if _prediction_info['special'] == 'DelqPct':\n",
    "                self.df = self.df[self.df['OpeningBalance'] != 0]\n",
    "                self.df['DelqPct'] = self.df.apply(lambda _row: (100 * _row['Delq']) / _row['OpeningBalance'], axis=1)\n",
    "\n",
    "                # Remove the old target and Delq from the dataframe\n",
    "                _cols = list(self.df.columns)\n",
    "                _cols.remove(self._target)\n",
    "                _cols.remove(\"Delq\")\n",
    "                self.df = self.df[_cols]\n",
    "                self._target = \"DelqPct\"\n",
    "            elif _prediction_info['special'] == 'ProfitLoss':\n",
    "                self.df['PaidForLoan'] = self.df.apply(\n",
    "                    lambda _row: _row['OpeningBalance'] - (_row['OpeningBalance'] * .15), axis=1)\n",
    "                self.df['PL'] = self.df.apply(lambda _row: _row['NumberPmnt'] * _row['Payment'] - _row['PaidForLoan'],\n",
    "                                              axis=1)\n",
    "\n",
    "    def remove_majority_null(self, threshold=.7):\n",
    "        total = len(self.df)\n",
    "        cols_to_drop = []\n",
    "        percents = []\n",
    "        for column in self.df.columns:\n",
    "            non_null_count = 0\n",
    "            for _idx, item in self.df[column].items():\n",
    "                if item is not None:\n",
    "                    non_null_count += 1\n",
    "            percent = float(non_null_count) / float(total)\n",
    "            if percent < threshold:\n",
    "                cols_to_drop.append(column)\n",
    "                percents.append((1 - percent) * 100)\n",
    "\n",
    "        self.df = self.df.drop(cols_to_drop, axis=1)\n",
    "        for _idx, column in enumerate(cols_to_drop):\n",
    "            print(\n",
    "                \"Dropping {column} since {percent}% of entires were Null\".format(column=column, percent=percents[_idx]))\n",
    "\n",
    "    def contains_null_or_is_categorical(self, series):\n",
    "        if self.is_categorical(series):\n",
    "            return True\n",
    "\n",
    "        for _idx, item in series.items():\n",
    "            if item is None:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type_series(series):\n",
    "        for _idx, item in series.items():\n",
    "            if item is not None:\n",
    "                return type(item)\n",
    "        return None\n",
    "\n",
    "    def is_categorical(self, series):\n",
    "        return self.get_type_series(series) == str\n",
    "\n",
    "    def get_correlated_entities(self, threshold=0.1, inplace=False):\n",
    "        corrs = dict()\n",
    "        print(\"Computing Entity Correlations\")\n",
    "        for col in tqdm(self.df.columns):\n",
    "            if col == self._target or self.contains_null_or_is_categorical(self.df[col]):\n",
    "                continue\n",
    "            corr, _ = pearsonr(self.df[col], self.df[self._target])\n",
    "            corrs[col] = abs(corr)\n",
    "\n",
    "        corrs = {_k: _v for _k, _v in sorted(corrs.items(), key=lambda item: item[1])}\n",
    "\n",
    "        goodCols = []\n",
    "\n",
    "        for col, score in corrs.items():\n",
    "            if score >= threshold:\n",
    "                goodCols.append(col)\n",
    "\n",
    "        print(goodCols)\n",
    "\n",
    "        if inplace:\n",
    "            goodCols.append(self._target)\n",
    "            self.df = self.df[goodCols]\n",
    "        else:\n",
    "            return self.df[goodCols]\n",
    "\n",
    "    def remove_outliers(self, lower_thresh=0.00, upper_thresh=0.999, ignore=None):\n",
    "        if ignore is None:\n",
    "            ignore = []\n",
    "        for col in self.df.columns:\n",
    "            if col in ignore or col == self._target or self.is_categorical(self.df[col]):\n",
    "                continue\n",
    "            percentiles = self.df[col].quantile([lower_thresh, upper_thresh]).values\n",
    "            self.df.loc[self.df[col] <= percentiles[0], col] = percentiles[0]\n",
    "            self.df.loc[self.df[col] >= percentiles[1], col] = percentiles[1]\n",
    "\n",
    "    def get_target_col(self):\n",
    "        return self.df[self._target]\n",
    "\n",
    "    def get_categorical_vars(self):\n",
    "        cats = []\n",
    "        for col in self.df.columns:\n",
    "            if self.is_categorical(self.df[col]):\n",
    "                cats.append(col)\n",
    "        return cats\n",
    "\n",
    "    def one_hot(self, inplace=False):\n",
    "        cat_attribs = self.get_categorical_vars()\n",
    "        full_pipeline = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), cat_attribs)],\n",
    "                                          remainder='passthrough')\n",
    "        encoder = full_pipeline.fit(self.df)\n",
    "        if inplace:\n",
    "            self.df = encoder.transform(self.df)\n",
    "        else:\n",
    "            return encoder.transform(self.df)\n",
    "\n",
    "    def one_hot_dummies(self, inplace=False):\n",
    "        cat_attribs = self.get_categorical_vars()\n",
    "        temp_df = self.df.copy(deep=True)\n",
    "        print(\"One Hot Encoding\")\n",
    "        for cat in tqdm(cat_attribs):\n",
    "            one_hot = pd.get_dummies(self.df[cat])\n",
    "            for i in one_hot.columns:\n",
    "                new_name = cat + i\n",
    "                new_name = new_name.replace(\"[\", \"\")\n",
    "                new_name = new_name.replace(\"]\", \"\")\n",
    "                new_name = new_name.replace(\"<\", \"\")\n",
    "                one_hot.rename(columns={i: new_name}, inplace=True)\n",
    "            temp_df = temp_df.drop(cat, axis=1)\n",
    "            temp_df = pd.concat([temp_df, one_hot], axis=1)\n",
    "\n",
    "        if inplace:\n",
    "            self.df = temp_df\n",
    "            self.move_col_to_end(self._target)\n",
    "        else:\n",
    "            return temp_df\n",
    "\n",
    "    def get_df_no_categoricals(self):\n",
    "        return self.df.drop(self.get_categorical_vars(), axis=1)\n",
    "\n",
    "    def get_df_highest_category(self, category, only_quantitative=True):\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        for item in self.df[category]:\n",
    "            counts[item] += 1\n",
    "\n",
    "        counts = {k: v for k, v in sorted(counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        highest = list(counts.keys())[0]\n",
    "        print(\"Training on {item}\".format(item=highest))\n",
    "\n",
    "        if only_quantitative:\n",
    "            return self.df[self.df[category] == highest].drop(self.get_categorical_vars(), axis=1)\n",
    "        else:\n",
    "            return self.df[self.df[category] == highest]\n",
    "\n",
    "    def standardize_categorical_column(self, col, inplace=False):\n",
    "        if inplace:\n",
    "            self.df[col] = self.df[col].apply(\n",
    "                lambda x: x.replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\",\n",
    "                                                                                                        \"\").replace(\"5\",\n",
    "                                                                                                                    \"\").replace(\n",
    "                    \"6\", \"\").replace(\"7\", \"\").replace(\"8\", \"\").replace(\"8\", \"\").replace(\" \", \"\"))\n",
    "        else:\n",
    "            return self.df[col].apply(\n",
    "                lambda x: x.replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\",\n",
    "                                                                                                        \"\").replace(\"5\",\n",
    "                                                                                                                    \"\").replace(\n",
    "                    \"6\", \"\").replace(\"7\", \"\").replace(\"8\", \"\").replace(\"8\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "    def print_cols(self):\n",
    "        for col in self.df.columns:\n",
    "            print(col)\n",
    "\n",
    "    def print_model_stats(self, y_test, preds):\n",
    "        # print(\"Preds\")\n",
    "        # print(preds[:10])\n",
    "        # print(\"Actual\")\n",
    "        # print(y_test[:10])\n",
    "        print(\"Confusion Matrix: \")\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print(\"Accuracy: \")\n",
    "        print(accuracy_score(y_test, preds))\n",
    "        print(\"F1: \")\n",
    "        print(f1_score(y_test, preds))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_feature_importance(_model):\n",
    "        _importance = _model.feature_importances_\n",
    "        # summarize feature importance\n",
    "        print(\"Feature Importance: \")\n",
    "        for i, v in enumerate(_importance):\n",
    "            print('Feature: %0d, Score: %.5f' % (i, v))\n",
    "        # plot feature importance\n",
    "        pyplot.bar([x for x in range(len(_importance))], _importance)\n",
    "        pyplot.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_name(_model, y_test, preds, model_name=\"\"):\n",
    "        _acc = accuracy_score(y_test, preds)\n",
    "        cur = datetime.now().strftime(\"%D:%H:%M\").replace(\"/\", \"-\").replace(\":\", \"_\", 1).replace(\":\", \"-\")\n",
    "\n",
    "        return \"{model_name}_{cur}-{acc}%.pkl\".format(model_name=model_name, cur=cur, acc=round(_acc, 3))\n",
    "\n",
    "    def train_model(self, _model, use_categoricals=False, print_stats=False, print_feature_importance=False, save=False,\n",
    "                    save_dir=\"../Models/\", model_name=\"model.pkl\"):\n",
    "        data = None\n",
    "        if use_categoricals:\n",
    "            data = self.df.copy(deep=True)\n",
    "        else:\n",
    "            data = self.get_df_no_categoricals().copy(deep=True)\n",
    "\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(data.iloc[:, :-1], data.iloc[:, -1:], test_size=0.2)\n",
    "\n",
    "        _model.fit(xtrain, ytrain)\n",
    "        self._model = _model\n",
    "\n",
    "        preds = _model.predict(xtest)\n",
    "\n",
    "        if print_stats:\n",
    "            self.print_model_stats(ytest, preds)\n",
    "\n",
    "        if print_feature_importance:\n",
    "            self.print_feature_importance(_model)\n",
    "\n",
    "        if save:\n",
    "            # filename = self.get_model_name(model, y_test, preds, model_name=model_name)\n",
    "            filename = model_name\n",
    "            print(\"Saving: {filename}\".format(filename=filename))\n",
    "            pickle.dump(_model, open(filename, 'wb+'))\n",
    "\n",
    "            # Save a model config file\n",
    "            conf = dict()\n",
    "            trained_entities = list(xtrain.columns)\n",
    "            conf['entities'] = trained_entities\n",
    "\n",
    "            with open(\"model_config.yaml\", 'w+') as yamlfile:\n",
    "                yaml.dump(conf, yamlfile)\n",
    "                print(\"Model config write successful\")\n",
    "\n",
    "    def run_model(self, model_name=\"model.pkl\", use_categoricals=False):\n",
    "        _model = pickle.load(open(model_name, 'rb'))\n",
    "        with open(\"model_config.yaml\") as f:\n",
    "            model_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        self._model = _model\n",
    "\n",
    "        input_data = None\n",
    "        if use_categoricals:\n",
    "            input_data = self.df.copy(deep=True)\n",
    "        else:\n",
    "            input_data = self.get_df_no_categoricals().copy(deep=True)\n",
    "\n",
    "        # Drop all entities not used during training\n",
    "        input_data = input_data[model_config['entities']]\n",
    "\n",
    "        # drop all rows containing na\n",
    "        input_data.dropna(how='any', inplace=True)\n",
    "\n",
    "        input_data[\"Prediction\"] = _model.predict(input_data)\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def get_scores(self, use_categoricals=False, plot = False):\n",
    "        # summarize feature importance\n",
    "        # importance = self._model.feature_importances_\n",
    "\n",
    "        df = self.df\n",
    "        if not use_categoricals:\n",
    "            df = self.get_df_no_categoricals()\n",
    "\n",
    "        importances = list(zip(self._model.feature_importances_, df.columns))\n",
    "        importances.sort(reverse=True)\n",
    "\n",
    "        scores = dict()\n",
    "        for score, entity in importances:\n",
    "            scores[entity] = score\n",
    "\n",
    "        mag = sum(scores.values())\n",
    "\n",
    "        for entity in list(scores.keys()):\n",
    "            scores[entity] = scores[entity]/mag * 100\n",
    "\n",
    "        names = list(scores.keys())\n",
    "        values = list(scores.values())\n",
    "\n",
    "        if plot:\n",
    "            plt.bar(range(len(scores)), values, tick_label=names)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.show()\n",
    "\n",
    "        for entity, score in scores.items():\n",
    "            print(\"{entity}: {score}\".format(entity=entity, score=score))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T15:14:13.479620Z",
     "end_time": "2023-04-19T15:14:13.502992Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping Discount since 100.0% of entires were Null\n",
      "Dropping CustomScore since 44.51843043995244% of entires were Null\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "server_info = config[\"server\"]\n",
    "\n",
    "dbl = DatabaseLoader(server_info)\n",
    "\n",
    "dbl.load_tables(config['tables'], config['ignore'])\n",
    "dbl.prune_for_target(config['predictions'])\n",
    "dbl.remove_majority_null() #removes columns where a majority of entries are null\n",
    "dbl.dropna() #removes all rows that have NA values. Do this after remove_majority_null\n",
    "dbl.remove_outliers(ignore = config['outliers']['ignore'])\n",
    "# dbl.one_hot_dummies(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T15:14:13.482934Z",
     "end_time": "2023-04-19T15:14:15.742197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(n_estimators = 1000)\n",
    "# dbl.train_model(model, print_stats = True, save = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T15:14:15.743661Z",
     "end_time": "2023-04-19T15:14:15.748916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recy: 7.292190261704016\n",
      "CreaditScore: 5.7612702587929805\n",
      "Discount: 5.170114406712299\n",
      "APR: 4.831536757753097\n",
      "Salary_x: 4.695700385939686\n",
      "LTV: 4.653481694703037\n",
      "TradelinesTotal: 4.6398807669855175\n",
      "OpeningBalance: 4.2471244501402206\n",
      "Account180DaysPastDue: 4.1968206312036305\n",
      "Apr: 4.031710554190731\n",
      "OpenTotal: 3.961529106566761\n",
      "VehicleYear: 3.9392328660477443\n",
      "PaidAccounts: 3.5765169140479807\n",
      "RentMortgage: 3.3112879104340514\n",
      "Mileage: 3.134649712768921\n",
      "PTI: 3.061295500225368\n",
      "InquiresTotal: 2.9178952484943843\n",
      "OpenAcctsInGoodStatus: 2.7176401479882912\n",
      "Mortgage: 2.7034946394489623\n",
      "LatePaymentHistory001260: 2.561296013230722\n",
      "LatePaymentHistory25Plus90: 2.475854287372601\n",
      "ClosedTotalTermsNotPaid: 2.11077766221738\n",
      "OpenTotalInBadStatus: 2.070010207066927\n",
      "Bankrupcies: 2.053524807712346\n",
      "EmploymentMonths: 2.0428997099792845\n",
      "OrigTerm: 2.0116570431096945\n",
      "LatePaymentHistory001230: 1.733340644091467\n",
      "CashDown: 1.7255420649193456\n",
      "Was180DaysPastDue: 1.0666788289765425\n",
      "Foreclosure: 0.5844716249441368\n",
      "NinetyDaysDelinquent: 0.5159698129968898\n",
      "Chageoff: 0.17520128686316963\n",
      "Reposessions: 0.029403792371807527\n",
      "TradelinesBalance: 0.0\n",
      "Term: 0.0\n",
      "Salary_y: 0.0\n",
      "Payment: 0.0\n",
      "MonthsAtHome: 0.0\n",
      "LatePaymentHistory25Plus60: 0.0\n",
      "LatePaymentHistory25Plus30: 0.0\n",
      "LatePaymentHistory132490Plus: 0.0\n",
      "LatePaymentHistory132460: 0.0\n",
      "LatePaymentHistory132430: 0.0\n",
      "LatePaymentHistory001290Plus: 0.0\n",
      "InquiriesLast6Months: 0.0\n",
      "DTI: 0.0\n",
      "Account150DaysPastDue: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Now pretend we load data with no labels (i.e. customers that we want to predict)\n",
    "testdbl = DatabaseLoader(server_info)\n",
    "# We still need to load all the tables that contain the entities\n",
    "testdbl.load_tables(config['tables'], config['ignore'])\n",
    "# We do not need to call prune for target (since the target should not exist in the database)\n",
    "\n",
    "# Now, we can run the model, which will append a prediction to customer\n",
    "testdf = testdbl.run_model(\"model.pkl\")\n",
    "\n",
    "# We can also get the entity scores from the model\n",
    "testdbl.get_scores()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T15:14:15.749616Z",
     "end_time": "2023-04-19T15:14:19.415033Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
